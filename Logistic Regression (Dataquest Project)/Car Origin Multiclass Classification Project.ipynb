{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0    70   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5    70   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0    70   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0    70   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5    70   \n",
      "\n",
      "   origin  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "[1 3 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"C:/Users/Lenovo 4/Desktop/Data Quest Folder/Logistic Regression/Multiclass Classification\")\n",
    "\n",
    "cars = pd.read_csv(\"auto.csv\")\n",
    "print(cars.head())\n",
    "unique_regions = cars[\"origin\"].unique()\n",
    "print(unique_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  displacement  horsepower  weight  acceleration  origin  cyl_3  cyl_4  \\\n",
      "0  18.0         307.0       130.0  3504.0          12.0       1      0      0   \n",
      "1  15.0         350.0       165.0  3693.0          11.5       1      0      0   \n",
      "2  18.0         318.0       150.0  3436.0          11.0       1      0      0   \n",
      "3  16.0         304.0       150.0  3433.0          12.0       1      0      0   \n",
      "4  17.0         302.0       140.0  3449.0          10.5       1      0      0   \n",
      "\n",
      "   cyl_5  cyl_6  ...  year_73  year_74  year_75  year_76  year_77  year_78  \\\n",
      "0      0      0  ...        0        0        0        0        0        0   \n",
      "1      0      0  ...        0        0        0        0        0        0   \n",
      "2      0      0  ...        0        0        0        0        0        0   \n",
      "3      0      0  ...        0        0        0        0        0        0   \n",
      "4      0      0  ...        0        0        0        0        0        0   \n",
      "\n",
      "   year_79  year_80  year_81  year_82  \n",
      "0        0        0        0        0  \n",
      "1        0        0        0        0  \n",
      "2        0        0        0        0  \n",
      "3        0        0        0        0  \n",
      "4        0        0        0        0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# if we set the prefix parameter to cyl, Pandas will pre-pend the column names to match the style we'd like\n",
    "\n",
    "dummy_cylinders = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n",
    "\n",
    "cars = pd.concat([cars, dummy_cylinders], axis=1)\n",
    "dummy_years = pd.get_dummies(cars[\"year\"], prefix=\"year\")\n",
    "cars = pd.concat([cars, dummy_years], axis=1)\n",
    "cars = cars.drop(\"year\", axis=1)\n",
    "cars = cars.drop(\"cylinders\", axis=1)\n",
    "print(cars.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this dataset, categorical variables exist in three columns, cylinders, year, and origin. The cylinders and year columns must be converted to numeric values so we can use them to predict label origin. Even though the column year is a number, weâ€™re going to treat them like categories. The year 71 is unlikely to relate to the year 70 in the same way those two numbers do numerically, but rather just as two different labels. In these instances, it is always safer to treat discrete values as categorical variables.\n",
    "\n",
    "We must use dummy variables for columns containing categorical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shuffled_rows = np.random.permutation(cars.index)\n",
    "shuffled_cars = cars.iloc[shuffled_rows]\n",
    "highest_train_row = int(cars.shape[0] * .70)\n",
    "train = shuffled_cars.iloc[0:highest_train_row]\n",
    "test = shuffled_cars.iloc[highest_train_row:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different methods of doing multiclass classification and in this mission, we'll focus on the one-versus-all method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unique_origins = cars[\"origin\"].unique()\n",
    "unique_origins.sort()\n",
    "\n",
    "models = {}\n",
    "features = [c for c in train.columns if c.startswith(\"cyl\") or c.startswith(\"year\")]\n",
    "\n",
    "for origin in unique_origins:\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    X_train = train[features]\n",
    "    y_train = train[\"origin\"] == origin\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    models[origin] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the one-vs-all approach, we're essentially converting an n-class (in our case n is 3) classification problem into n binary classification problems. For our case, we'll need to train 3 models:\n",
    "\n",
    "A model where all cars built in North America are considered Positive (1) and those built in Europe and Asia are considered Negative (0).\n",
    "A model where all cars built in Europe are considered Positive (1) and those built in North America and Asia are considered Negative (0).\n",
    "A model where all cars built in Asia are labeled Positive (1) and those built in North America and Europe are considered Negative (0).\n",
    "Each of these models is a binary classification model that will return a probability between 0 and 1. When we apply this model on new data, a probability value will be returned from each model (3 total). For each observation, we choose the label corresponding to the model that predicted the highest probability.\n",
    "\n",
    "We'll use the dummy variables we created from the cylinders and year columns to train 3 models using the LogisticRegression class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_probs = pd.DataFrame(columns=unique_origins)\n",
    "testing_probs = pd.DataFrame(columns=unique_origins)  \n",
    "\n",
    "for origin in unique_origins:\n",
    "    # Select testing features.\n",
    "    X_test = test[features]   \n",
    "    # Compute probability of observation being in the origin.\n",
    "    testing_probs[origin] = models[origin].predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "113    1\n",
      "114    1\n",
      "115    1\n",
      "116    1\n",
      "117    1\n",
      "Length: 118, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "predicted_origins = testing_probs.idxmax(axis=1)\n",
    "print(predicted_origins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While each column in our dataframe testing_probs represents an origin we just need to choose the one with the largest probability. We can use the Dataframe method .idxmax() to return a Series where each value corresponds to the column or where the maximum value occurs for that observation. We need to make sure to set the axis paramater to 1 since we want to calculate the maximum value across columns. Since each column maps directly to an origin the resulting Series will be the classification from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
